{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47908da5-27db-412c-946f-781ab7491b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transformation from Bronze to Silver\n",
    "This script focus on :\n",
    "- Cleaning the data from NULL values\n",
    "- Converting the data format\n",
    "- Standardizing the column names\n",
    "- Create OrderDateCalendar dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c337ffb5-89a7-4e41-8a51-8bba10de4b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp, date_format, current_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bde84d91-0fea-47de-b0a0-58aade302ae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Cleaning and standardizing the data format of Order table and transfer to silver container\n",
    "\n",
    "path = '/mnt/bronze/Orders/Orders.parquet'\n",
    "df = spark.read.format('parquet').load(path)\n",
    "df=df.filter((df['orderdate']!='NULL'))\n",
    "df=df.filter((df['item_code']!='NULL'))\n",
    "df=df.filter((df['CategoryNo']!='NULL'))\n",
    "df = df.withColumn('orderdate', date_format(from_utc_timestamp(df['orderdate'].cast(TimestampType()), \"UTC\"), \"yyyy-MM-dd\"))\n",
    "# df = df.withColumn(\"OrderDate\",  date_format(to_date(df['orderdate'], \"MM/dd/yy HH:mm\"), \"yyyy-MM-dd\"))\n",
    "df = df.withColumnRenamed(\"item_code\", \"ItemCode\").withColumnRenamed(\"vendor_id\", \"VendorID\").withColumnRenamed(\"orderdate\", \"OrderDate\")\n",
    "display(df)\n",
    "output_path = '/mnt/silver/Orders/'\n",
    "df.write.format('delta').mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce65e4e2-834f-4c13-996d-8fb2f30e52aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Standardizing the column name of Vendor table and transfer to silver container\n",
    "\n",
    "path = '/mnt/bronze/Vendor/Vendor.parquet'\n",
    "df = spark.read.format('parquet').load(path)\n",
    "df = df.withColumnRenamed(\"vendorname\", \"VendorName\")\n",
    "display(df)\n",
    "output_path = '/mnt/silver/Vendor/'\n",
    "df.write.format('delta').mode(\"overwrite\").save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b9be104-9350-4989-8880-2782e7544d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Cleaning and standardizing column name of OrderItem table and transfer to silver container\n",
    "\n",
    "path = '/mnt/bronze/OrderItems/OrderItems.parquet'\n",
    "df = spark.read.format('parquet').load(path)\n",
    "df = df.withColumnRenamed(\"itemid\", \"ItemID\").withColumnRenamed(\"item\", \"ItemDescription\").withColumnRenamed(\"quantity\", \"Quantity\").withColumnRenamed(\"itemno\", \"ItemNumber\").withColumnRenamed(\"list_price\", \"ListPrice\").withColumnRenamed(\"discount_price\", \"DiscountPrice\").drop(\"subtotal\")\n",
    "display(df)\n",
    "output_path = '/mnt/silver/OrderItem/'\n",
    "df.write.format('delta').mode(\"overwrite\").save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e87d970f-e8ea-4417-a148-ec29bb93ac25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Transfer ItemCode table and transfer to silver container\n",
    "\n",
    "path = '/mnt/bronze/ItemCode/ItemCode.parquet'\n",
    "output_path = '/mnt/silver/ItemCode/'\n",
    "df = spark.read.format('parquet').load(path)\n",
    "df.write.format('delta').mode(\"overwrite\").save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31ee66a-80cd-4fd9-80f8-0974a7ea46d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create OrderDateCalendar table and transfer to silver container\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = spark.createDataFrame([(1,)], [\"id\"])\n",
    "\n",
    "df1 = df.withColumn(\n",
    "    \"OrderDate\", \n",
    "    F.explode(F.expr(\"sequence(to_date('2020-01-01'), to_date('2025-12-31'), interval 1 day)\"))\n",
    ")\n",
    "output_path = '/mnt/silver/OrderDateCalendar/'\n",
    "dforderdatecalendar = df1.withColumn(\"Year\", year(\"OrderDate\")).withColumn(\"Year\", year(\"OrderDate\")).withColumn(\"Month\",month(\"OrderDate\")).withColumn(\"Day\",dayofmonth(\"OrderDate\")).withColumn(\"Week\", weekofyear(\"OrderDate\")).withColumn(\"MonthName\", date_format('OrderDate', 'MMMM')).withColumn(\"DayName\", date_format('OrderDate', 'EEEE'))\n",
    "dforderdatecalendar.write.format('delta').mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(output_path)\n",
    "dforderdatecalendar.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze to silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
